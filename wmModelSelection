#!/usr/bin/env groovy

import durbin.weka.* 
import durbin.weka.FoldSets
import durbin.util.*
import java.lang.management.*

err = System.err // sugar
def bExceptionOccurred = false

// Get the command line options, print help if needed. 
options = ParseOptions(args)

// Expand the configuration file into a list of experiments. 
// Clinical file name is passed in so config can handle "ALL" attribute if present.
// otherwise it is ignored...
def totalExperiments
if (options.config){
	err.print "\nReading experiments from config..."
	allExperiments = new WekaMineConfig(options.config,options.clinical)
	err.println "${allExperiments.size()} experiments read from config. "
	//experiments = allExperiments[options.experimentRange]
	totalExperiments = allExperiments.size()
	experiments = allExperiments
}else if (options.flatConfig){
	
	err.println "DISABLED TEMPORARIALY... need to reconcile code for this and normal cfg..."
	System.exit(1);
	
	err.print "Reading experiment list..."
	experiments = new WekaMineConfig()
	if (options.experimentsPerJob){
		// If we're writing out a job list, don't actually read the whole file, just determine it's size
		totalExperiments = experiments.readFlat(options.flatConfig,(0..0))
	}else{
		totalExperiments = experiments.readFlat(options.flatConfig,options.experimentRange)
	}
	err.println "${experiments.size()} experiments read out of $totalExperiments file."
}

printHeapSpace(options.verbose)

FoldSets foldsets;
if (options.foldsetsFile){
	err.print "Reading foldset ${options.foldsetsFile}..."
	foldsets = new FoldSets(options.foldsetsFile)	
	err.println " ${foldsets.size()} foldsets read."
}

// Write out a flat list of experiments if requested...
if (options.flatExperimentOut){
	println WekaMineResult.expHeadingStr
	System.out.withWriter{w->
		allExperiments.each{experiment->
			w << experiment.toOutputString()
			w << "\n"
		}
	}
	System.exit(0)
}

// Write out jobs and quit if that is requested...
if (options.experimentsPerJob){
	scriptFile = getClass().protectionDomain.codeSource.location.path	
	WekaMine.writeClusterJobList(args,totalExperiments,options.experimentsPerJob,scriptFile)
	System.exit(0)
}

err.println ""
// Create output files, a summary output, a features output, and a samples output.

def from = options.experimentRange.getFrom()
def to = options.experimentRange.getTo()
def rangeStr = "$from.$to"

fileOut = "${options.output}${rangeStr}.summary.tab" as String
err.println "Creating output file $fileOut"
out = newOverwrite(fileOut)

featuresOut = "${options.output}${rangeStr}.features.tab" as String
err.println "Creating features output file $featuresOut"
//fout = new File(featuresOut)
fout = newOverwrite(featuresOut)

samplesOut = "${options.output}${rangeStr}.testsamples.tab" as String
err.println "Creating samples output file $samplesOut"
//sout = new File(samplesOut)
sout = newOverwrite(samplesOut)

// Output information about training set if requested. 
def tout
if (options.evalTraining){
	trainingOut = "${options.output}${rangeStr}.trainingsamples.tab" as String	
	err.println "Creating samples output file $trainingOut"
	tout = new File(trainingOut)	
}

err.println ""

// Print a heading to the output files unless heading is suppressed (say, for cluster jobs)
if (!options.suppressHeading){
	
	if (options.validationFraction){
		out << WekaMineResults.getFormattedSummaryHeading() 
		out << ",holdoutAcc"
		out << "\n"
	}else{	
		out << WekaMineResults.getFormattedSummaryHeading() << "\n"
	}
	fout << WekaMineResults.getFormattedSummaryHeading() << "\n"
	sout << WekaMineResults.getFormattedSummaryHeading() << "\n"
}

// Read the data (expression, CNV, whatever) and the clinical from tab delimited files
if (options.dataMayContainNominalValues){
	err.println "Reading data with nominal values."
	data = WekaMine.readFromTable(options.data)
}else{
	err.println "Reading numeric data."
	data = WekaMine.readNumericFromTable(options.data)
}
//data = TableFileLoader.readNumeric(options.data,options.data,"\t");
clinical = WekaMine.readFromTable(options.clinical,instancesInRows = options.clinicalInstancesInRows)

(0..< clinical.numAttributes()).each{i->
	attr = clinical.attribute(i)
	values = (0..<attr.numValues()).collect{attr.value(it)}
	err.println "attribute ${attr.name()} has values: ${values}"	
}

// Remove everything except a subset if a list of samples to use is given.
if (options.sampleSubset){
	data = WekaMine.subsetInstances(data,options.sampleSubset)
	clinical = WekaMine.subsetInstances(clinical,options.sampleSubset)
}

printHeapSpace(options.verbose)		

// Occasionally we're given a data file that is a huge superset of the clinical file
// as a consession to RAM, remove all instances from data except those in the clinical file..
err.print "Removing data instances not in clinical, before size = ${data.numInstances()}..."
clinicalNames = clinical.attributeValues("ID")
data = WekaMine.subsetInstances(data,clinicalNames)
err.println "done. After data.size=${data.numInstances()}"
System.gc()
printHeapSpace(options.verbose)		

// Use seed or randomize with clock...
def rng = new Random();
if (experiments.params.cvSeed == -1){
	rng = new Random();
}else{
	rng = new Random(experiments.params.cvSeed)
}

err.println "DEBUG: "+options.experimentRange
//experiments[options.experimentRange].eachWithIndex{exp,idx-> 
//	err.println "DEBUG experiment $idx"
//}

// Perform each experiment described in the experiment spec...
//experiments[options.experimentRange].eachWithIndex{exp,idx->  Experiments now subseted before here...
experiments[options.experimentRange].eachWithIndex{exp,idx-> 
	jobIdx = idx+options.experimentRange.getFrom()
	err.println "------------------------------------------------"
	err.println "Experiment: $jobIdx \t Class attribute: ${exp.classAttribute}"
	try{
						
		// Creates a wmModelSelection pipeline...
		pipeline = new WekaMine(data,clinical,exp,experiments.params)		
		pipeline.dataName = options.data
		
		printHeapSpace(options.verbose)		
		
		// if the filter is supervised, need to combine data first then filter...
		if (exp.filter instanceof weka.filters.SupervisedFilter){
			// Combines data and single class attribute from clinical into one set of instances...
			//instances = pipeline.createInstancesFromDataAndClinical(data,clinical,exp.classAttribute)	
			instances = pipeline.createInstancesFromDataAndClinical(data,clinical)	
			err.println "Combined instances: "+instances.numInstances()			
			instances = pipeline.applyUnsupervisedFilter(instances)	
			printHeapSpace(options.verbose)										
			System.gc()
		}else{		
			// Apply unsupervised filter if any...  Filter could be MultiFilter implementing
			// several transforms combined. 		Note: As it is, the data is filtered before 
			// the class attribute has even been added.  You can filter it after the class attribute
			// has been added if the classAttribute is set and the filter option is set to ignore class.b	
			instances = pipeline.applyUnsupervisedFilter(data)	
		
			//err.println "DEBUG ${instances.attributeValues('BRAF')}"
						
			printHeapSpace(options.verbose)	
						
			// Combines data and single class attribute from clinical into one set of instances...
			//instances = pipeline.createInstancesFromDataAndClinical(data,clinical,exp.classAttribute)	
			instances = pipeline.createInstancesFromDataAndClinical(instances,clinical)	
			err.println "Combined instances: "+instances.numInstances()
			System.gc()
		}
		
		
		//err.println "DEBUG ${data.attributeValues('BRAF')}"
		//err.print "DEBUG CLASS ATTRX: "
		//err.println "${instances.attributeValues('OVk4_Proliferative_vs_notProliferative')}"
		
		printHeapSpace(options.verbose)
		
		// Remove instances in the holdout set.  Since holdouts are stratified, there is a 
		// different holdout for each class attribute. 		WARNING: Stratification is only 
		// valid for a particular class attribute discretization, so mixing multiple discretizations
		// in config will result in holdouts that are no longer stratified. 
		if (options.holdoutSet){			
			err.print "Removing ${options.holdoutSet[exp.classAttribute].size()} holdout samples out of ${instances.numInstances()} for ${exp.classAttribute}..."
			instances = WekaMine.removeInstances(instances,options.holdoutSet[exp.classAttribute])
			err.println "done. ${instances.numInstances()} instances remaining."
		}				
		
		//err.println "instances.size="+instances.size()		
		//err.println "DEBUG ${instances.attributeValues('BRAF')}"
		//err.println "DEBUG CLASS ATTR: ${instances.attributeValues('OVk4_Proliferative_vs_notProliferative')}"
				
		// Clean up instances:
		// * remove useless attributes
		// * if not allowed, remove instances with negative class values
		// * remove instances with missing class values	
		instances = pipeline.cleanUpInstances(instances)
					
		// Discretize the class attribute...
		(instances,cutoffs) = pipeline.discretizeClassAttribute(instances)	
		
		// Remove censored samples that can't be put into a discretized class... 
		// KJD Should these be removed?? Shouldn't their class just be set to unknown? 
		if (experiments.params.censoredAttribute){
			instances = pipeline.removeUnclassifiableCensoredSamples(instances,clinical,experiments.params.censoredAttribute)
		}				

		// Create an attribute selected classifier from the given experiment description..
		if (exp.numAttributes > instances.numAttributes()){
			exp.numAttributes = -1;
		}				
		asClassifier = pipeline.createAttributeSelectedClassifier()
	
		for(repetition = 0;repetition < options.repetitions;repetition++){
			err.println "Experiment repetition: $repetition"			
			// Perform the cross validation
			err.println "ATTRIBUTE SELECTION: AttSel:  ${exp.attrEvalStr}; ${exp.attrSearchStr} numAttrs: ${exp.numAttributes}"
			err.println "CLASSIFIER: ${exp.classifierStr}"
			err.println "Crossvalidating model on ${instances.numInstances()} instances ${exp.classAttribute} ..."			
				
			Evaluation2 eval 
			if (options.foldsetsFile){	
				eval = pipeline.crossValidateModelWithGivenFolds(asClassifier,instances,foldsets,options.evalTraining)
			}else{
				eval = pipeline.crossValidateModel(asClassifier,instances,options.cvFolds,rng,options.evalTraining)
			}
	
			err.print "Appending test results..."
			pipeline.appendSummary(out,jobIdx,instances)
			pipeline.appendFeatures(fout,jobIdx,instances,options.maxFeaturesOut as Integer)
			pipeline.appendTestSamples(sout, jobIdx,instances)
			err.println "done."
				
			if (options.evalTraining){			
				err.print "Append training results..."
				pipeline.appendTrainingSamples(tout,jobIdx,instances)
				err.println "done."
			}		
		
			err.println pipeline.eval.toMatrixString("\nCV TEST SAMPLE CONFUSION MATRIX:")		
			err.println "AUC: ${eval.weightedAreaUnderROC()} for ${exp.classAttribute}"
		} // ============ END REPETITIONS
		
		// Report memory usage and encourage garbage collection...
		err.println "\n"
		//err.println getHeapSpace()
		
		printHeapSpace(options.verbose)			
		
		filteredData = null
		instances = null
		asClassifier = null
		eval = null
		err.print "Collecting garbage..."
		System.gc()
		err.println "done."
		//err.println getHeapSpace()
			
	  err.println "==========================================\n"	  
	}catch (Exception e){
		err.println "\n\n"
		err.println "Exception occurred on experiment $jobIdx"
		err.println e		
		
		err.println "Continuining with remaining experiments in this batch. "
		// Save info about exception..
		bExceptionOccurred = true
	}
}	

if (bExceptionOccurred) {
	err.println "An exception occurred during this run."
	System.exit(1)
}else System.exit(0)
	
	// File doesn't have a close (as opposed to writer)... Are writes to File guaranteed to flush? 
	//out.close()
	//fout.close()
	//sout.close()
	//if (options.evalTraining) tout.close()
	
	// Quit with non-zero (positive) on exception. 
		




//============================================================================
// 


/****************************************************
* Parse the command line options, checking validity, printing help if needed. 
*/ 
def ParseOptions(args){
	parser = new Parser(description: '''
 wmModelSelection performs a model selection experiment to find the best set of algorithms
 and parameters for a classification problem. 

 wmModelSelection takes a tab delimited data file in attributes (rows) x samples (cols) format, 
 and a clinical file, also in attributes (rows) x samples (cols) format, and evaluates 
 the set of experiments specified in an experimental configuration file.  Classification is 
 performed on the samples that occur in both the data and clinical file (intersection). 
 A list of summary lines in CSV format will be APPENDED to the end of the output file.  
 Since this will often be run in a cluster setting with many separate output files combined
 into one file, the heading can optionally be omitted. 

 Instances with missing class values are removed.   Attributes that do not vary at all 
 are removed.  Instances with negative class values are optionally removed.  

 The actual experiments and experimental options are provided by a domain specific language (DSL)
 config file (-c). Documentation for this config file can be found here:  

 https://github.com/jdurbin/wmModelSelection/wiki/Wekamine-config

 Note that the config file can specify a number of attributes to use in attribute 
 selection, or -1 to auto-select (the details of which varies from attribute selection 
 algorithm to attribute selection algorithm). The config file option actually determines 
 how many attributes the classifier sees.  The -m/maxFeaturesOut option merely determines 
 the maximum number of selected attributes (and their scores) to save in the output and 
 does not affect what attributes the classifier will see. 

 An optional validation set can be held out from the cross-validation portion of the 
 experiment.  Each model will be trained on all data not in the evaluation set and 
 evaluated on the evaluation set.  The evaluation accuracy will be saved with roc and
 other parameters, as a convenience, but should not be used to pick the best model. The
 best model should be picked based on roc or other CV parameters, and the evaluation 
 accuracy used only after-the-fact to verify that the model does predict something on 
 an independent set.  That is, one should pretend that the evaluation set is only 
 applied to the one best model, after picking the best model.  It is applied to all models
 simply to avoid having to communicate the hold-out set between wmModelSelection and subsequent
 model selection/building/evaluation steps. 
	
	 
	Written by: James Durbin (kdurbin@ucsc.edu)

	 Example:

	 wmModelSelection  \\
	 -d data/paradigm_results.txt -i data/collisonclinical.small.tab \\
	 -c exp/cfgExample2.txt -r 0,19 -s include.samples -o results/expout.tab

	''');

	parser.with{

	  required 'o','output',[description: 'File where output should go. Generates three files *.summary.tab, *.features.tab, and *.samples.tab']
	  required 'd','data', [description: 'Data file in attribute (row) by samples (col) format.']
	  required 'i','clinical', [description: 'Clinical file in attribute (row) by samples (col) format.']
	
		optional 'k','experimentsPerJob',[description: 'When specified, wmModelSelection does not run but instead outputs a list of wmModelSelection commands, k experiments each, for the cluster.']
		flag 'F','flatExperimentOut',[default:false,description: 'Does not perform experiments but merely writes their description to stdout.']

	  optional 'c','config',[description: 'Configuration file.  One of -c or -C required. ']  
		optional 'C','flatConfig',[description: 'A flat configuration file, one experiment per line. One of -c or -C required.']  

		optional 'v','cvFolds',[default:10,description: 'Cross-validation folds.',validate:{it as int}]
		optional 'x','repetitions',[default:1,description: 'Repetitions of each k-fold experiment',validate:{it as int}]

		optional 'V','verbose',[description: 'Output verbose info, like RAM usage, etc.']
		optional 'f','foldsetsFile',[default:null,description: "File specifying the cross validation foldsets to use."]
	  optional 'm','maxFeaturesOut',[default: 100,description: "Maximum number of ranked features to output from cross-validation or full-set if classifier is 'None'"]
		optional 'S','holdoutSet',[default:null,description: "File listing samples not to use.  All other samples will be included from experiments",
			validate:{
				if (it != null){				
					// Read in the samplesToOmit... 
					samplesToOmitByClassAttribute = [:]
					new File(it).splitEachLine("\t"){fields->
						classAttr = fields[0]
						samplesToOmit = fields[1..-1]
						samplesToOmitByClassAttribute[classAttr] = samplesToOmit
					}
					return(samplesToOmitByClassAttribute)
				}else {
						return(it)							
				}
			}
		]
		optional 's','sampleSubset',[default:null,description: "File listing samples to use.  All other samples will be excluded from analysis.",
			validate:{
				if (it != null){				
					// Read in the samplesToUse... 
					def samplesToUse = []
					new File(it).eachLine{samplesToUse << it}
					return(samplesToUse)
				}else {
					return(it)							
				}
			}
		]

	  optional 'r','experimentRange',[default: "0,-1", description: 'Range of experiments to run (e.g. -r 54,67, mainly for cluster splits)',
			// Convert it to a proper range. Default is all inclusive range. 
			validate:{								
		 		experimentStart = (it.split(","))[0] as int
		 		experimentEnd = (it.split(","))[1] as int 
			  range = (experimentStart..experimentEnd)
				return(range)
			}]			
			
		//optional 'v','validationFraction',[default:0.0,description: 'Fraction of samples to withold from cross-validation portion of experiment, evaluating only at end on fully trained model.',
		//validate:{it as double}]		

		flag 't','evalTraining',[default:false,description:'If set, the classifier is evaluated on the training data as well as the test data in cross validation, output in *.trainingsamples.tab.  NOTE: This will slow down execution, possibly significantly.' ]
		flag 'R','clinicalInstancesInRows',[default:false,description:'If set, clinical data will be assumed to be one instance per row. Default one instance per column.']		
		flag 'H','suppressHeading',[default:false,description: 'Suppress heading for summary.tab. (to make easier to cat cluster job outputs)']
		flag 'N','dataMayContainNominalValues',[default:false,description: 'Option to read datafile in as mixed numeric/nominal.  Slower than default all numeric. ']
	  flag 'h','help',[default:false,description: 'Print script help.']
	}

	def options
	try{
	  options = parser.parse(args)
	
		if (!(options.config || options.flatConfig)){
			System.err.println "A config file must be specified with either -c or -C."
			System.err<<parser.usage
			System.exit(1)
		}		
		
		if (options.config && options.flatConfig){
			System.err.println "Specify only ONE of options -c or -C "
			System.err<<parser.usage
			System.exit(1)
		}
		
	}catch(Exception e){
	  System.err << parser.usage
	  System.exit(1)
	}	
	return(options)
}


def accuracy(results,holdoutInstanceIDs,clinical,model){	
	classValues = model.classValues
	//err.println "\t$classValues"
	double correct = 0;
	double total = 0;
	holdoutInstanceIdxs = clinical.nameListToIndexList(holdoutInstanceIDs)
	holdoutInstanceIDs.eachWithIndex{instanceID,i->
		instIdx = holdoutInstanceIdxs[i]
		clinicalInst = clinical[instIdx]
		clinicalValue = clinicalInst[model.className]
		distForInstance = results[i]
		//err.println "\t$instanceID\t$distForInstance\t$clinicalValue"
		maxIdx = maxIdx(distForInstance)
		if (classValues[maxIdx] == clinicalValue){
			correct++
		}
		total++
	}	
	return(correct/total)
}


def maxIdx(dist){
	def maxval = -999
	def maxidx = -1
	dist.eachWithIndex{val,i->
		if (val > maxval){
			maxval = val
			maxidx = i
		}		
	}	
	return(maxidx)		
}

static void printHeapSpace(verbose){
	if (!verbose) return;
	
	MemoryMXBean mxmemory = ManagementFactory.getMemoryMXBean();
	MemoryUsage heap = mxmemory.getHeapMemoryUsage();
	System.err.println("\t---- Heap Committed: "+heap.getCommitted()/1000000000.0);
	System.err.println("\t---- Heap Used: "+heap.getUsed()/1000000000.0);
	System.err.println("\t---- Heap Max: "+heap.getMax()/1000000000.0);
}


def getHeapSpace(){
	mxmemory = ManagementFactory.getMemoryMXBean()
	heap = mxmemory.getHeapMemoryUsage()
	return("$heap")
}

def newOverwrite(fileName){
	f = new File(fileName)
	if (f.exists()){
		f.delete()
		f.createNewFile()
	}
	return(f)
}


