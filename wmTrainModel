#!/usr/bin/env groovy

import grapnel.weka.* 
import grapnel.util.*

import weka.classifiers.meta.FilteredClassifier;
import weka.filters.unsupervised.attribute.RemoveType;
import weka.core.SerializationHelper;
import weka.filters.supervised.attribute.*
import weka.filters.*
import weka.attributeSelection.*

WekaAdditions.enable() // Enable meta-class additions to weka API

err = System.err // sugar

// Get the command line options, print help if needed. 
options = ParseOptions(args)

// Expand the configuration file or wekaMine results into a list of experiments.
def experiments = [] 
def wmr
if (options.config){
	err.print "\nReading experiments from config..."
	allExperiments = new WekaMineConfig(options.config,options.clinical)
	err.println "${allExperiments.size()} experiments read from config. "
	totalExperiments = allExperiments.size()
	experiments = allExperiments
		
	//experiments = new WekaMineConfig(options.config)
	params = experiments.params
}else if (options.resultsFile){
	wmr = new WekaMineResults(options.resultsFile)
			
	if (options.topModel){	
		err.print "Determining top model..."
		// Find the top result...
		maxStat = -999;
		maxIdx = -1;
		for(int i = 0;i < wmr.size();i++){
				if (wmr[i].roc > maxStat){
					maxStat = wmr[i].roc
					maxIdx = i
				}
		}
		err.println "done."
		// Create an experiment from it...
		topExperiment = wmr[maxIdx].toExperiment()
		err.println "Top Model: "+topExperiment
		experiments = []
		experiments << topExperiment
	}else{
		experiments = wmr.toExperiments()
	}
	params = [:]	
}else if (options.experimentString){
	headingMap = WekaMineResult.defaultHeadingMap()
	specdelimiter = ","
	experiment = new ExperimentSpec(options.experimentString,headingMap,specdelimiter)
	experiments << experiment
	params = [:]
}else if (options.experimentStringFile){
	headingMap = WekaMineResult.defaultHeadingMap()
	expString = new File(options.experimentStringFile).text	
	specdelimiter = "\t"
	experiment = new ExperimentSpec(expString,headingMap,specdelimiter)
	experiments << experiment 
	params=[:]
}else{
	System.err.println "How did we get here?  Must specify one of -c, -R, or -E to define the model. Run with -h for options. "
	System.exit(1)
}

err.println "${experiments.size()} experiments read."

// Write out jobs and quit if that is requested...
if (options.experimentsPerJob){
	scriptFile = getClass().protectionDomain.codeSource.location.path	
	WekaMine.writeClusterJobList(args,experiments.size(),options.experimentsPerJob,scriptFile)
	System.exit(0)
}

FoldSets holdout;
if (options.holdoutFile){
	err.print "Reading holdout ${options.holdoutFile}..."
	holdout = new FoldSets(options.holdoutFile)	
	err.println " ${holdout.size()} holdouts read."
}


// Read the data (expression, CNV, whatever) and the clinical from tab delimited files
rawdata = WekaMine.readNumericFromTable(options.data)
clinical = WekaMine.readFromTable(options.clinical)
def data

def rawNullData
if (options.nullBackgroundFile){
	rawNullData = WekaMine.readNumericFromTable(options.nullBackgroundFile)
}

experiments.each{e->
	err.println "Experiment class: ${e.classAttribute}"		
}
err.println "clinical attributes: "+clinical.attributeNames()


// Perform each experiment described in the experiment spec...
experiments[options.experimentRange].eachWithIndex{exp,idx-> 
	jobIdx = idx+options.experimentRange.getTo()
	err.println "=================================="
	err.println "Experiment: $jobIdx"
	try{
		
		// Creates a wekaMine pipeline...
		pipeline = new WekaMine(rawdata,clinical,exp,params)					
		
		if (options.holdoutSet){			
			err.print "Removing ${options.holdoutSet[exp.classAttribute]?.size()} holdout samples for ${exp.classAttribute}..."
			
			if (options.holdoutSet.keySet().contains(exp.classAttribute)){			
				data = WekaMine.removeInstances(rawdata,options.holdoutSet[exp.classAttribute])
			}else{
				msg = "Holdout set doesn't contain expected attribute: ${exp.classAttribute}"
				msg += "\nHoldout contains keys: ${options.holdoutSet.keySet()}"
				throw new Exception(msg)
			}
			err.println "done."
		}else{
			data = rawdata  
		}				
		
		// QN Filter has to be applied after attribute selection for performance
		// and compatibility with wmClassify
		def instances = data
		if (exp.filter != null){
			if (!(exp.filter instanceof grapnel.weka.QuantileNormalizationFilter)){		
				instances = pipeline.applyUnsupervisedFilter(instances)	
			}
		}
								
		// Combines data and single class attribute from clinical into one set of instances...
		// KJD: apparently, the order is not guaranteed to match between instances and data...
		// I don't depend on the order, but it's something to keep in mind...
		instances = pipeline.createInstancesFromDataAndClinical(instances,clinical,exp.classAttribute)	

		// Clean up instances:
		// * remove useless attributes
		// * if not allowed, remove instances with negative class values
		// * remove instances with missing class values	
		instances = pipeline.cleanUpInstances(instances)

		// Discretize the class attribute...
		(instances,cutoffs) = pipeline.discretizeClassAttribute(instances)
				
		err.println "cutoffs: $cutoffs"		
				
		// Remove ID attribute, since attribute evaluators and classifiers choke on it...
		instances = AttributeUtils.removeInstanceID(instances)
		
		// Apply the attribute selection algorithm to instances...			
		instances = pipeline.selectAttributes(instances)	
		
		// Doing this after attribute selection lets us assume that the 
		// training set and the test set have the same attributes which 
		// allows the algorithm to be faster.  
		if (exp.filter != null){
			if (exp.filter instanceof grapnel.weka.QuantileNormalizationFilter){	
				instances = pipeline.applyUnsupervisedFilter(instances)	
			}
		}
													
		err.print("Build classifier ${exp.classifierStr}...")
		model = new WekaMineModel(instances,exp.classifier,cutoffs,exp.filter)
		
		// If we have the results of experiments, add performance info to the model
		if (options.resultsFile){
			result = wmr[jobIdx]
			model.roc = result.roc
			model.tp1 = result.tp1
			model.fp1 = result.fp1
			model.tn1 = result.tn1
			model.fn1 = result.fn1
		}
		
		//exp.classifier.buildClassifier(instances);		
		err.println("done.")				

		// Create an object to hold all the info we need to classify new instances with 
		// this model...	
		//model = new WekaMineModel(instances,exp.classifier,cutoffs)
		//model = new WekaMineModel(instances,exp.classifier,cutoffs,exp.filter)

		// If we have a nullModelFile, process the instances, otherwise use instances. 		
		if (options.nullBackgroundFile){
			//nullData = pipeline.readNumericFromTable(options.nullBackgroundFile)	
			def nullData = rawNullData
			if (exp.filter != null){
				if (!(exp.filter instanceof grapnel.weka.QuantileNormalizationFilter)){		
					nullData = pipeline.applyUnsupervisedFilter(rawNullData)	
				}
			}
			
			// Remove ID attribute, since attribute evaluators and classifiers choke on it...
			nullData = AttributeUtils.removeInstanceID(nullData)			
			
			
			
			// Make it match attribute selected instances.  
			// Note that instances does not contain ID at this point. 			
			// Also note that this function is ignorant about class attributes, so we handle that separately. 
			attributesMinusClass = instances.attributeNames() - instances.className()
			nullData = InstanceUtils.createInstancesToMatchAttributeList(nullData,attributesMinusClass)
						
			// Apply filter
			if (exp.filter != null){
				if (exp.filter instanceof grapnel.weka.QuantileNormalizationFilter){	
					nullData = pipeline.applyUnsupervisedFilter(nullData)	
				}
			}
						
			//err.println "DEBUG before: instances.attributes: "+nullData.numAttributes()
			
			// classifiers choke if there isn't an empty class attribute...
			nullData = WekaMine.createEmptyClassAttribute(nullData,model.className,model.classValues)
			nullData.setClassName(instances.className())
						
			err.print "Adding ${nullData.numInstances()} background samples..."			
			model.addNullSamples(nullData)
			err.println "done."
		}else{
			err.println "Null permutation size: "+instances.numInstances()+" instances x "+instances.numAttributes()+" attributes."
			nullData = instances
		}

		// Create and save a bootstrap null model by permuting attributes, if requested...
		if (options.nullModelSamples){					
			err.println "Computing null model with ${options.nullModelSamples} bootstrap samples."
			err.println "Note: This can take awhile. Also note that the null model will expand the size of the saved model."
			samples = options.nullModelSamples as int
			iterations = (double)samples/((double)nullData.numInstances())
			iterations = Math.ceil(iterations)
			err.println "This will require $iterations passes through permuted instances."
			model.computeBootstrapNullModel(nullData,iterations)
			err.println "done."
		}		

		// Write out the model file...
		def rootName = options.modelFileName
		def cleanClass = exp.classAttribute.replaceAll(" ","_")
		if (options.useClassName) {
			rootName = "$rootName"+cleanClass
			outFile = "${rootName}.wmm"
		}else{
			outFile = "${rootName}_${jobIdx}.wmm"
		}
		
		println "outFile: "+outFile
		
		
		// Before we save the model, see what we can do to save space	
		cleanupModel(model)
				
		
		err.print "Save model to $outFile ..."
		SerializationHelper.write(outFile,model)
		err.println "done."
		
		// Encourage garbage collection maybe...
		instances = null;
		samples = null;
		model = null;
			
	}catch (Exception e){
		err.println e
	}
}

/***
*	Weka Logistic regression (class LogisticBase) saves a copy of the training data
*   This isn't needed for classification so we can safely remove it. 
*   Similarly, the Kernel, (class CachedKernel) saves a copy of the training data. 
*   Nonlinear kernels need this to build the kernel matrix, but linear kernels do not. 
*   So we remove it from linear kernels.   
*  
*   Given the space cost of nonlinear SMO kernels, I should require a minimum 
*   performance gain in order to use it over a more space efficient model. 
* 
*/ 
def cleanupModel(model){
	def classifier = model.classifier
	def baseClassifier
	if (classifier instanceof weka.classifiers.meta.FilteredClassifier){
		baseClassifier = classifier.getClassifier()
		if (baseClassifier instanceof grapnel.weka.AttributeSelectedClassifier2){
			baseClassifier = baseClassifier.getClassifier()
		}						
	}else if (classifier instanceof grapnel.weka.AttributeSelectedClassifier2){
		baseClassifier = classifier.getClassifier()
	}else{	
		baseClassifier = classifier
	}
	
	switch(baseClassifier){
		case {it instanceof weka.classifiers.functions.SimpleLogistic}:
			baseClassifier.m_boostedModel.cleanup()
		break;
		
		case {it instanceof weka.classifiers.functions.SMO}:
			if (baseClassifier.m_KernelIsLinear){
				// KJD Breaks if nonbinary				
				// KJD I'm really quite confused why these kernels are showing up as null. 
				def c = baseClassifier.m_classifiers[0][1]
				if (c == null) err.println "ERROR: Null base classifier. "
				if (c.m_kernel == null) {
					err.println "Null kernel? [0][1]"
					err.println "kernel [0][0]: "+baseClassifier.m_classifiers[0][0]
				}else c.m_kernel.m_data=null
			}
		break;
	}	
}


//============================================================================
// 

/****************************************************
* Parse the command line options, checking validity, printing help if needed. 
*/ 
def ParseOptions(args){
	parser = new Parser(description: '''
	wmTrainModel takes a tab delimited data file in attributes (rows) x samples (cols) format, 
	and a metadata file, also in attributes (rows) x samples (cols) format, and trains 
	a classifier either on the classifier(s) specified in the config file or the classifiers 
	specified in a wekaMine results summary .tab file.  The resulting model(s) are written to 
	a binary output file for later use. Note: attributes reported after filtering include 
	class attribute.  Optionally, a null model may be generated by permuting the attribute values 
	and classifying the permuted set.  This null model will be saved in the .wmm file.  
	
	Documentation for the config file can be found here:  

	https://github.com/jdurbin/wekaMine/wiki/Wekamine-config

	Written by: James Durbin (kdurbin@ucsc.edu)

	Example:

	wmTrainModel  -d lin2008.tab -i lin2008.clinical.tab -c cfg.txt -o lin2008.wekaMine.wmm

	''');
	
	parser.with{
		required 'o','modelFileName',[description: 'Root file name where output should go.']
		required 'd','data', [description: 'Data file in attribute (row) by samples (col) format.']
		required 'i','clinical', [description: 'Clinical file in attribute (row) by samples (col) format.']

		optional 'c','config',[description: 'Configuration file']  
		optional 'R','resultsFile',[description: 'Use given wekaMine summary results to build classifier(s). Will save one model per result in file.']

		optional 'B','nullModelSamples',[description: 'Compute bootstrap null model (label permutations) and save that in model file. Option specifies how many points to compute.']		
		optional 'n','nullBackgroundFile',[description: 'A data file in row x samples format that will be used to generate a background null model. If combined with -B, these samples will be permuted also.']
		optional 'S','holdoutSet',[default:null,description: "File listing samples not to use.  All other samples will be included in training model.",
			validate:{
				if (it != null){				
					// Read in the samplesToOmit... 
					samplesToOmitByClassAttribute = [:]
					new File(it).splitEachLine("\t"){fields->
						classAttr = fields[0]
						samplesToOmit = fields[1..-1]
						samplesToOmitByClassAttribute[classAttr] = samplesToOmit
					}
					return(samplesToOmitByClassAttribute)
				}else {
						return(it)							
				}
			}
		]
				
		flag 'T','topModel',[default: false,description: 'With -R option, will output a model corresponding to the best result in the results file.']

		optional 'E','experimentString',[description: 'A single experiment string. In single quotes, comma separated: attrEval,attrSearch,numAttributes,classifier.  Use keyword "none" to omit a field.']
		optional 'e','experimentStringFile',[description: 'Exp string in file for batch systems that do not like quotes']		
	  	optional 'r','experimentRange',[default: "0,-1", description: 'Range of experiments to run (e.g. -r 54,67, mainly for cluster splits)',
			// Convert it to a proper range. Default is all inclusive range. 
			validate:{								
		 		experimentStart = (it.split(","))[0] as int
		 		experimentEnd = (it.split(","))[1] as int 
			  range = (experimentStart..experimentEnd)
				return(range)
			}]			
		optional 'k','experimentsPerJob',[description: 'When specified as last option, wmSaveModel does not run but instead outputs a list of wmSaveModel commands, k experiments each, for the cluster.']
		flag 'N','useClassName',[default:false,description: 'Tack on class name as part of modelFileName.']
		flag 'h','help',[default:false,description: 'Print script help.']
	}

	def options
	try{
	  options = parser.parse(args)
	
		if ((!options.config && !options.resultsFile && !options.experimentString && !options.experimentStringFile) || (options.help)){
			System.err.println "Must specify one of -c, -R, or -E to define the model. Run with -h for options. "
			System.err<<parser.usage
			System.exit(1)
		}
		
		if (options.topModel && (!options.resultsFile)){
			System.err.println "-T topModel option must be used with -R resultsFile option. "
			System.err<<parser.usage
			System.exit(1)
		}
		
	
	}catch(Exception e){
	  System.err << parser.usage
	  System.exit(1)
	}	
	
	return(options)
}


/***
wmSaveModel -d data/vijver2002.tab -i data/vijver2002.nominal.tab -o results/nominal -T -R results/nominal_none-1.0.summary.tab 

real	0m8.498s

1000 null model points (4 iterations)		real	0m14.095s		568,270

*/
